Background:
Pre-trained transformer language models have shown success in various domains, but their application to generative materials design is novel. Navigating the vast chemical space to discover new materials is challenging and requires intelligent computational algorithms.

Challenge:
Exploring the uncharted chemical space for new materials via experimental tinkering or computational models is costly and limited. Intelligent generative models are needed to efficiently navigate the design space and discover novel materials compositions.

Current Status:
Pre-trained transformer language models have not been widely used for the generative design of inorganic materials compositions. Existing approaches rely on crystal structure prediction algorithms and data mining for materials discovery.

Method:
Seven modern transformer language models were trained on datasets from materials databases to generate chemically valid material compositions. The datasets included varied samples from different databases, and the models were evaluated based on composition validity and novelty.

Result:
The transformer-based models demonstrated high performance in generating chemically valid and novel material compositions. Among the models, MT-GPTJ showed the best performance in terms of validity and generation speed.

Conclusion:
The study shows the potential of deep learning language models for generative materials design by efficiently exploring the chemical space and discovering novel materials compositions. The models outperformed traditional random generation algorithms and showed promise in material discovery.

Outlook:
Future research may focus on refining the training datasets, optimizing hyperparameters, and improving the generation capabilities of the transformer-based models. Integrating crystal structure prediction algorithms and experimental validation can further enhance the practical application of these models in materials design.